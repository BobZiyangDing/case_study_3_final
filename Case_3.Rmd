---
title: "Case Study 3: Election Prediction"
author: "Bob Ding, Becca Erenbaum, Grace O'Leary, Rena Zhong"
date: "11/1/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width = 7, fig.height = 5)
options(digits = 4)
```

```{r packages, include=F}
library(tidyverse)
library(R2jags)
library(ggplot2)
library(knitr)
library(matrixStats)
library(gridExtra)
```

# 1. Introduction

The outcome of the 2016 election not only stunned the nation, but also sent shockwaves through the statistical and polling communities. Over the course of the election year in 2016 up until the week of the election, poll predictions of Hillary Clinton's likelihood of beating Donald Trump ranged from 71%- 99% probability [1]. So when Trump beat these odds, the polling industry lost a lot of trust from the general public [2].

This small, specialized industry that is the political polling business, has a great deal of influence on how the election is portrayed in the news media, voter decisions, and candidate partisan policy initiatives. [1]. Million dollar decisions on advertising and campaign strategy are dictated by polls. 

Polls conducted early in the election year are a weak predictor of election outcomes because the general public has paid less attention to the race and is less knowledgeable of the candidates' platforms. Voters that tend to sway between parties often report that they are undecided, however these are arguably the most important opinions in predicting the election outcomes. [3] As the election nears, polls become more accurate, but that is where historical prediction models have been lacking -- they fail to take into account the differential accuracy in poll results. Historical models, regression based models that rely on outcomes from past elections and structural factors, predict election outcomes at a single point in time which renders high levels of uncertainty [4]. 

Drew A. Linzer developed a dynamic bayesian forecasting model to predict the U.S. presidential election at the national and state level that combines these historical models with everchanging poll updates. Linzer's model uses hierarchical specification to handle states being polled on different days and takes into account sampling errors of the polls and national campaign effects [4]. 

In this report we aim to:

* Predict the outcome of the presidential election and the electoral college vote using the Linzer model to predict swing state outcomes in combination 

* Predict whether the US Senate remains in Republican control by using an adaptation of the Linzer model and FiveThirtyEight Senate poll data for each state

* Predict the outcomes of all 13 NC Congressional elections using Linzer model with input from FiveThirtyEight Senate poll data for North Carolina along with our model that predicts who will vote in North Carolina

* Predict the outcome of the NC Senate election and the associated uncertainty using the Linzer model.

Taking into account the anomaly that was the 2016 election, we chose to use the Linzer model to best account for slight and unexpected changes leading up to election day. In Section 2 of this report we will discuss datasets used for all tasks, including a brief exploratory data analysis. In Section 3, we formulate models and methodologies that answer all the research questions. Section 4 will present model diagnostic and sanity check validation of prediction. Then, in section 5 we present the major results of our analysis. Section 6 will be focused on conducting sensitivity analysis to test data imputation hypothesis and prior choices.

# 2. Data Source and EDA

```{r load-data, include=FALSE, warning=FALSE}
president_polls_2020 <- read_csv("2020 US presidential election polls - all_polls.csv") %>%
    mutate(days_to_election = as.Date("2020/11/03","%Y/%m/%d" ) - as.Date(end.date, "%m/%d/%Y"),
         state = ifelse(state == "--","Overall",state),
         y = biden/(biden + trump)*100)

house_polls <- read_csv("house_polls.csv")
senate_polls <- read_csv("senate_polls.csv")
ncvhis_Statewide_small <- read_rds("ncvhis_Statewide_small.rds")
```

## 2.1 Description of Data

In order to answer these questions, we used a total of four datasets: senate polls, house polls, 2020 US presidential election polls, and North Carolina voter registration history snapshot dataset. Both the senate and house polls dataset comes from the fivethirtyeight website [5], while the presidential election polls dataset comes from the Economist website [6].Senate and house polls have 38 variables and 4061 and 2655 observations respectively. The presidential election polls have 1447 observations and 17 variables which are: poll state, pollster, sponsor, start and end date, entry time, number of observations, population, method, Biden, Trump, Biden margin, other, undecided, URL, include, and notes.

The North Carolina voter history dataset contains information on the 2016 elections and about how voters voted (method, election, county, party affiliation, precinct). This will help us model and identify potential voters in 2020 (Interim report). On top of the created model, we used the 2020 voter registration snapshot dataset to identify potential voters, and use this information as additional input to model house polls results.

## 2.2 Exploratory Data Analysis

To get a basic understanding of the four datasets we were working with, we went through the data to understand what we were working with. In the senate and the house polls, we explored the variables of: methodology of the poll, the state in which the poll was conducted, which party the candidate was, and the percentage of the poll. In the presidential election polls, we explored similar variables of: method of poll and state, in addition to the margin between Biden and Trump. Because all three of these datasets had similar variables, we are able to compare them to each other. In Figure A, we can see how the methods of polling were distributed between the house, senate, and presidential polls. In Figure B, we can see how each poll's distribution of which state was polled, and in Figure C, we can see how the house and senate polls' candidate party are different from each other. Finally, in Figure D, we can see the distribution of the margin between Biden and Trump.

To explore the North Carolina voter history dataset, we looked at variables such as method, county, and party affiliation of the voter. We can see in Figure E that the majority of North Carolina voters are from two counties, Wake and Mecklenberg. Party affiliation and the method that the voters used are included in our appendix.

```{r eda-methods, echo=FALSE, fig.cap="Figure A: Poll Methodology"}
house_method <- ggplot(house_polls, mapping = aes(x=methodology)) +
  geom_bar() +
  labs(title = "Poll Methodology Distribution", subtitle = "House Polls",
       x = "Method Type") +
  theme(axis.text.x = element_text(angle = 45))
house_method

senate_method <- ggplot(senate_polls, mapping = aes(x=methodology)) +
  geom_bar() +
  labs(title = "Poll Methodology Distribution", subtitle = "Senate Polls",
       x = "Method Type") +
  theme(axis.text.x = element_text(angle = 45))
senate_method

presidential_method <-ggplot(president_polls_2020, mapping = aes(x=mode)) +
  geom_bar() +
  labs(title = "Poll Methodology Distribution", subtitle = "Presidential Polls",
       x = "Method Type") +
  theme(axis.text.x = element_text(angle = 45))
presidential_method

grid.arrange(house_method, senate_method, presidential_method, ncol=2)
```

```{r eda-state, echo=FALSE, fig.cap="Figure B: Polls State Distribution"}
house_state <-ggplot(house_polls, mapping = aes(x=state)) +
  geom_bar() +
  labs(title = "Poll State Distribution", subtitle = "House Polls",
       x = "State") +
  theme(axis.text.x = element_text(angle = 45))
house_state

senate_state <-ggplot(house_polls, mapping = aes(x=state)) +
  geom_bar() +
  labs(title = "Poll State Distribution", subtitle = "Senate Polls",
       x = "State") +
  theme(axis.text.x = element_text(angle = 45))
senate_state

presidential_state <-ggplot(president_polls_2020, mapping = aes(x=state)) +
  geom_bar() + 
  labs(title = "Poll State Distribution", subtitle = "Presidential Polls",
       x = "State") +
  theme(axis.text.x = element_text(angle = 45))
presidential_state
```

```{r eda-house-senate-polls, echo=FALSE, fig.cap="Figure C: Candidate Party Polls"}
house_candidate <-ggplot(house_polls, mapping = aes(x=candidate_party)) +
  geom_bar() +
  labs(title = "Poll Candidate Party Distribution", subtitle = "House Polls",
       x = "Party Affiliation") +
  theme(axis.text.x = element_text(angle = 45))
house_candidate

senate_candidate <-ggplot(senate_polls, mapping = aes(x=candidate_party)) +
  geom_bar() +
  labs(title = "Poll Candidate Party Distribution", subtitle = "Senate Polls",
       x = "Party Affiliation") +
  theme(axis.text.x = element_text(angle = 45))
senate_candidate
```

```{r eda-biden, echo=FALSE, fig.cap="Figure D: Biden Margin"}
biden_margin <-ggplot(president_polls_2020, mapping = aes(x=biden_margin)) +
  geom_bar() +
  labs(title = "Biden Margin", x = "Range")
biden_margin
```

# 3. Model Formulation

## 3.1 Model for Question 1

In this section, we build a model to answer our first research question: Predict the outcome of the presidential election and the electoral college vote using the Linzer model to predict swing state outcomes in combination. We follow the route of Linzer and build our model. Specifically, the motivation is that we want to model Joe Biden's percentage of EC vote in the state specific level of detail. We believe that throughout time, each state's preference for Joe Biden is randomly varying and follows a random walk process. Besides, each state's uncertainty of their attitude towards Joe Biden is also state-widely different. We make the assumption that such uncertainty is constant throughout time. Therefore, we create the following extension of Linzer Model.

$$
\begin{aligned}
y_{k} & \sim N\left(\beta_{i[k] t[k]}, (\sigma_y^2)_{i[k]}\right) \\
\text { for } t>1: \beta_{i t} & \sim N\left(\beta_{i, t-1}, (\sigma_{\beta}^{2})\right) \\
\text { for } t=1: \beta_{i 1} & \sim N\left( \mu_0, \sigma^{2}_0\right) \\
(\sigma_y^2)_{i[k]} &\sim \text{InvGamma}(\nu_y, \tau_y)\\
(\sigma_\beta^2) &\sim \text{InvGamma}(\nu_\beta, \tau_\beta)\\\\
\mu_0 &\sim N(50, 17)\\
\sigma_0^2 &\sim \text{InvGamma}(\frac{1}{2}, \frac{1}{2})\\
\nu_y &\sim \text{Uni}(0, 100)\\
\tau_y &\sim \text{Uni}(0, 100)\\
\nu_\beta &\sim \text{Uni}(0, 100)\\
\tau_\beta &\sim \text{Uni}(0, 100)\\
\end{aligned}
$$

Where $k$ index the polls, $i$ index the states, and $t$ index the date. $i[k]$ represents the $k^{th}$ poll's corresponding state index, and $t[k]$ represents the $k^{th}$ poll's corresponding date index. We model samples from the economist poll data of 2020 and therefore are able to sample the posterior distribution of 1) Joe Biden's chance of winning, and 2) total electoral vote. The sampled posterior distribution y (percentage of supportance within each state) will be calculated by comparing to rounding by 50% to simulate the "winner takes all" procedure. After such adjustment, we can simply multiply the number of electoral college votes of each state to obtain a posterior distribution of electoral college votes of the entire nation. Thus, 2) can be obtained. By comparing to 270 again provides the probability of Joe Biden being elected.


```{r data president, include=F}
president_polls_2020 <- subset(president_polls_2020, days_to_election < 200)
president_polls_2020 <- president_polls_2020 %>% 
  filter(days_to_election <= 200, state %in% c("FL","GA","IA","NC","OH","TX","AZ","MI","MN","NV","NH","PA","WI"))

states <- president_polls_2020$state %>% unique

y <- president_polls_2020$y
r <- match(president_polls_2020$state,states)
t <- president_polls_2020$days_to_election + 1 #WHY PLUS ONE?

N_polls <- y %>% length
N_states <- states %>% length
N_days <- t %>% max

jags_data <- list(y=y,t=t,r=r,
                  N_polls=N_polls,N_states=N_states,N_days=N_days)

```

```{r mv_model-president, include=F}
model_mv <- function(){
  for(k in 1:N_polls){
    y[k] ~ dnorm(p[k],1/sigma2_y[r[k]]) #note no longer binomial
    p[k] = theta[r[k],t[k]]
  }
  for(j in 2:N_days){
    theta[1:N_states,j] ~ dmnorm(theta[1:N_states,j-1],Phi)
  }
  Phi ~ dwish(I_states,N_states+1) #fill in wishart parameters, google JAGS wishart distribution should turn it up
  Sigma = inverse(Phi)
  #which, Phi or Sigma is the covariance and which is the precision? 
  
  #optional: theta[1:N_states,1] ~ dmnorm(mu0,s0) #define mu0 and s0 in your jags_data object
  
  #Use your hierarchical prior for sigma2_y from before 
  for(j in 1:N_states){
      sigma2_y[j] = 1/sigma2_y_inv[j]
      sigma2_y_inv[j] ~ dgamma(nu_y,nu_y*tau_y) 
      
      theta[j,1] ~ dnorm(mu0,pow(sigma2_0,-1))
  }
  nu_y ~ dunif(0,100)
  tau_y ~ dunif(0,100)
  
  nu_beta ~ dunif(0,100)
  tau_beta ~ dunif(0,100)
  
  mu0 ~ dnorm(50,pow(7.5,-2))
  sigma2_0 = 1/sigma2_0_inv
  sigma2_0_inv ~ dgamma(.5,.5)
}
```


```{r run-model-president,eval=TRUE,cache=TRUE, include=F}
jags_data <- list(y=y,t=t,r=r,
                  N_polls=N_polls,N_states=N_states,N_days=N_days)

jags_data$I_states <- diag(N_states)

#be sure to add your added parameters to parameters.to.save
jags_sims_mv <- jags(data = jags_data,model.file = model_mv,parameters.to.save = c("theta","Sigma",
                                                                                "p","sigma2_y"),
                  n.iter = 10000)
```

```{r voting state plot, include=F, message=F}
poll_plot_data <- tibble(y=jags_data$y,t=jags_data$t %>% as.integer(),state = states[jags_data$r])



beta_plot_data <- lapply(1:N_states,function(index){
  sims <- jags_sims_mv$BUGSoutput$sims.list$theta[ , index , ]
  data.frame(state = states[index],t=1:N_days,mean=colMeans(sims),lb=apply(sims,2,quantile,probs=.025),ub=apply(sims,2,quantile,probs=.975))
}) %>% 
  bind_rows()

swing_state_traj <- left_join(beta_plot_data,poll_plot_data) %>% 
  ggplot(aes(x=t)) + 
  geom_line(aes(y=mean)) + 
  geom_ribbon(aes(ymin = lb,ymax = ub),alpha = .2) +
  geom_point(aes(y=y)) + 
  scale_x_reverse() + 
  facet_wrap(~ state)



```



```{r plot implied variance, echo=F}
poll_plot_data$p <- jags_sims_mv$BUGSoutput$mean$p
poll_plot_data$sigma2y <- jags_sims_mv$BUGSoutput$mean$sigma2_y[jags_data$r]
poll_plot_data$binom_v <- (poll_plot_data$p)*(100-poll_plot_data$p)/ president_polls_2020$number.of.observations


# 
president_var <- poll_plot_data %>%
  ggplot(aes(x=binom_v)) +
  geom_density() +
  geom_vline(aes(xintercept = sigma2y),color = "red") +
  facet_wrap(~ state)


```

```{r winning probability, include=F, eval=T}
sims <- jags_sims_mv$BUGSoutput$sims.list$theta[,,1]
colnames(sims) <- states
swing_state_win_prob <- as.data.frame(sims %>% {.>50} %>% colMeans())
names(swing_state_win_prob) <- "Winning Probability"
swing_state_win_prob <- kable(swing_state_win_prob, caption = "Swing State Winning Probability")

swing_state_share_interval_president <- kable(colQuantiles(sims, probs=c(0.025, 0.5, 0.975)), caption = "Swing State Share Percentage Interval Estimate (Biden)")

```

```{r ec vote distribution, echo=F}
#in other states: Biden: 232-13, Trump: 306-29-18-15-10-20
# c_ex_votes <- 232-13; t_ex_votes <- 306-29-18-15-10-20
ec_votes <- ((sims>50) %*% diag(c(10,20, 29,38,16,16,15, 9, 18, 11, 6, 6, 4))) %>% rowSums() + 210
ec_vote_mean <- mean(ec_votes)
ec_vote_qt <- quantile(ec_votes, probs=c(0.025, 0.975))
ec_vertical_lines <- c(ec_vote_qt[1], ec_vote_mean, ec_vote_qt[2])
ec_dis <- ggplot() + aes(ec_votes)+ geom_histogram(binwidth=5) + ggtitle("Electorial College Vote Share Distribution") + geom_vline(xintercept = ec_vertical_lines )
ec_interval <- kable(quantile(ec_votes, probs = c(0.025, 0.975) ), caption="EC Vote Total (Biden)")

swing_state_prob <- mean({ec_votes > 270})
```



```{r some random functions, include=F}
reorder_cormat <- function(cormat){
  # Use correlation between variables as distance
  dd <- as.dist((1-cormat)/2)
  hc <- hclust(dd)
  cormat <-cormat[hc$order, hc$order]
}
get_upper_tri <- function(cormat){
  cormat[lower.tri(cormat)]<- NA
  return(cormat)
}

get_heatmap <- function(cormat){
  cormat %>% 
    cov2cor() %>% 
    reorder_cormat %>% 
    get_upper_tri %>% 
    reshape2::melt(na.rm = TRUE) %>% 
    ggplot(aes(Var2, Var1, fill = value))+
    geom_tile(color = "white")+
    scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                         midpoint = 0, limit = c(-1,1), space = "Lab", 
                         name="Pearson\nCorrelation") +
    theme_minimal()+ # minimal theme
    labs(x="",y="") + 
    theme(axis.text.x = element_text(angle = 45, vjust = 1, 
                                     size = 12, hjust = 1))+
    coord_fixed()
}
```

```{r cross state correlation, echo=F}
cor_sims <- array(NA,dim = jags_sims_mv$BUGSoutput$sims.list$Sigma %>% dim)
for(i in 1:dim(cor_sims)[1]){
  cor_sims[i,,] <- cov2cor(jags_sims_mv$BUGSoutput$sims.list$Sigma[i,,])
}
cor_mean <- apply(cor_sims,c(2,3),mean)
colnames(cor_mean) <- states
rownames(cor_mean) <- states

#full heatmap
get_heatmap(cor_mean)
```




## 3.2 Model for Question 2, 4

In this section, we're answering question 2 and 4 jointly: Predict whether the US Senate remains in republican control and predict the outcome of the NC Senate election. Similarly, using the above model with exactly the same notation, and by switching Economist dataset to FiveThirtyEight senate polls data, the exact inference procedure can be produced. After modeling and predicting the Republican's support percentage, we can predict whether the Republican or Democrat party wins the state majority. This procedure includes the state of North Carolina. Furthermore, by summing the posterior samples of Republican senators in each state on election day, we can obtain the posterior distribution of US Senate remains in Republican Party's control.

```{r make us senator data, include=F}
senator <- read_csv("senate_polls.csv")
senator$days_to_election = as.Date(senator$election_date, "%m/%d/%Y") - as.Date(senator$end_date, "%m/%d/%Y")
senator <- senator %>% filter(candidate_party %in% c("DEM","REP"))
senator$state <- ifelse(senator$state == "NA","US",senator$state)
senator <- senator %>% filter(days_to_election <= 100)
states <- senator$state %>% unique

senator$y = ifelse(senator$candidate_party == "REP", senator$pct, 100-senator$pct) # support ratio for rep
senator$r <- match(senator$state, states)
senator$t <- senator$days_to_election + 1 #WHY PLUS ONE?

senator_y<- senator$y
senator_r<- senator$r
senator_t<- senator$t

N_polls <- senator_y %>% length
N_states <- states %>% length
N_days <- senator_t %>% max


```


```{r mv_model-senator, include=F}
model_senator <- function(){
  for(k in 1:N_polls)
  {
    y[k] ~ dnorm(p[k],1/sigma2_y[r[k]]) #note no longer binomial
    p[k] = theta[r[k],t[k]]
  }
  for(j in 2:N_days)
  {
    theta[1:N_states,j] ~ dmnorm(theta[1:N_states,j-1],Phi)
  }
  Phi ~ dwish(I_states,N_states+1) #fill in wishart parameters, google JAGS wishart distribution should turn it up
  Sigma = inverse(Phi)
  #which, Phi or Sigma is the covariance and which is the precision? 
  
  #optional: theta[1:N_states,1] ~ dmnorm(mu0,s0) #define mu0 and s0 in your jags_data object
  
  #Use your hierarhciacl prior for sigma2_y from before 
  for(j in 1:N_states){
      sigma2_y[j] = 1/sigma2_y_inv[j]
      sigma2_y_inv[j] ~ dgamma(nu_y,nu_y*tau_y) 
      
      theta[j,1] ~ dnorm(mu0,pow(sigma2_0,-1))
  }
  nu_y ~ dunif(0,100)
  tau_y ~ dunif(0,100)
  
  nu_beta ~ dunif(0,100)
  tau_beta ~ dunif(0,100)
  
  mu0 ~ dnorm(50,pow(7.5,-2))
  sigma2_0 = 1/sigma2_0_inv
  sigma2_0_inv ~ dgamma(.5,.5)
}
```

```{r run jags senator,eval=TRUE,cache=TRUE, include=F}

senator_jags_data <- list(y=senator_y,t=senator_t,r=senator_r,
                  N_polls=N_polls,N_states=N_states,N_days=N_days)

senator_jags_data$I_states <- diag(N_states)

#be sure to add your added parameters to parameters.to.save
jags_sims_senator <- jags(data = senator_jags_data,
                          model.file = model_senator,
                          parameters.to.save = c("theta","Sigma","p","sigma2_y"),
                          n.iter = 10000)
```

```{r plot senator for all state, eval=T, include=F}
senator_poll_plot_data <- tibble(y=senator_jags_data$y,
                                 t=senator_jags_data$t %>% as.integer(),
                                 state = states[senator_jags_data$r])

senator_beta_plot_data <- lapply(1:N_states,function(st){
  sims <- jags_sims_senator$BUGSoutput$sims.list$theta[,st,]
  data.frame(state = states[st],
             t=1:N_days,mean=colMeans(sims),
             lb=apply(sims,2,quantile,probs=.025),
             ub=apply(sims,2,quantile,probs=.975))}) %>% 
  bind_rows()

senator_trend <- left_join(senator_beta_plot_data,senator_poll_plot_data) %>% 
  ggplot(aes(x=t)) + 
  geom_line(aes(y=mean)) + 
  geom_ribbon(aes(ymin = lb,ymax = ub),alpha = .2) + 
  geom_point(aes(y=y)) + 
  scale_x_reverse() + 
  facet_wrap(~ state)


```

```{r plot senator prob for all state, eval=T}
senator_poll_plot_data$p <- jags_sims_senator$BUGSoutput$mean$p
senator_poll_plot_data$sigma2y <- jags_sims_senator$BUGSoutput$mean$sigma2_y[senator_jags_data$r]
senator_poll_plot_data$binom_v <- (senator_poll_plot_data$p)*(100-senator_poll_plot_data$p)/ senator$sample_size


senator_var <- senator_poll_plot_data %>%
  ggplot(aes(x=binom_v)) +
  geom_density() +
  geom_vline(aes(xintercept = sigma2y),color = "red") +
  facet_wrap(~ state)


```

```{r all state senator expectation, include=F}
senator_sims <- jags_sims_senator$BUGSoutput$sims.list$theta[,,1]
colnames(senator_sims) <- states
senator_all_state_prob <- as.data.frame(senator_sims %>% {.>48} %>% colMeans())
names(senator_all_state_prob) <- "Winning Probability"
senator_all_state_prob <- kable(senator_all_state_prob, caption = "Senator All State Winning Probability")
swing_state_share_interval <- kable(colQuantiles(senator_sims, probs=c(0.025, 0.5, 0.975)), caption = "Senator All State Vote Share Percentage Interval Estimate")


```

```{r answer for North Carolina, include=F}
temp <- as.data.frame( t(senator_sims %>% {.>48} %>% colMeans()) )

longnames <- NULL
interval_nc_senate <- quantile(as.data.frame(senator_sims)$`North Carolina`,  probs = c(0.025, 0.5, 0.975))
nc_senate_share_qt <- kable(interval_nc_senate, caption="North Carolina Vote Share Interval Estimate")
mean_nc_senate_prob <- temp$`North Carolina`
nc_senate_share <- ggplot() + aes(interval_nc_senate)+ geom_histogram() + geom_density(fill="blue", alpha=0.3) + ggtitle("NC Tom Tillis Voter Share Distribution") + geom_vline(xintercept = mean(interval_nc_senate))

tom_win_prob <- mean({as.data.frame(senator_sims)$`North Carolina` > 48})

```

```{r prob senator remains in control of republican, include=F}

total_rep_senate <-  as.numeric(( { senator_sims >48 } * 2) %>% rowSums() +2)

senate_dis <- ggplot() + aes(total_rep_senate)+ geom_histogram(binwidth=2) + ggtitle("Distribution of Senators In REP of Whole Nation") + geom_vline(xintercept = mean(total_rep_senate))

rep_senate_prob <- mean( { total_rep_senate > 48}   ) 
```


## 3.3 Model for Question 3

In this section, we answer research question 3: Predict the outcomes of all 13 NC Congressional elections. The dataset we've used is the FiveThirtyEight house poll data. However, the dataset's sample sizes on each NC congressional district are small. This is due to the fact house polls happen less frequently compared to either senate polls or presidential polls. Besides, not all congressional districts are competitive because there is no term limit for representatives. FiveThirtyEight has only recorded poll history data for competitive districts including districts 2,3,7,8,9,11,13. Thus, correlation between non-competitive districts cannot be captured without inputting extra datasets. Thus, we concatenated this section's modeling procedure with the output of the interim reports' "who vote" results. 

Using the model to predict who will vote in 2020, we ran this binary output model on all the registered voters in NC to predict whether they'll vote or not. In the NC voter registration profile 2020 snapshot dataset, there is a column indicating party affiliation of each potential voter. Thus, we can predict the percentage of Republican voters for each congressional district and use this percentage as an imputed value for polls percentage. In this way, we're essentially imputing polling results for non-competitive districts, which can be also fed into the Linzer Model defined below. Such imputation is reasonable but bold. Thus, sensitivity analysis of imputed percentage will be further explored later in section ********

Below is the model we're using for this task:

$$
\begin{aligned}
y_{k} & \sim N\left(\beta_{i[k] t[k]}, (\sigma_y^2)_{i[k]}\right) \\
\text { for } t>1: \beta_{i t} & \sim N\left(\beta_{i, t-1}, (\sigma_{\beta}^{2})\right) \\
\text { for } t=1: \beta_{i 1} & \sim N\left( \mu_0, \sigma^{2}_0\right) \\
(\sigma_y^2)_{i[k]} &\sim \text{InvGamma}(\nu_y, \tau_y)\\
(\sigma_\beta^2) &\sim \text{InvGamma}(\nu_\beta, \tau_\beta)\\\\
\mu_0 &\sim N(50, 17)\\
\sigma_0^2 &\sim \text{InvGamma}(\frac{1}{2}, \frac{1}{2})\\
\nu_y &\sim \text{Uni}(0, 100)\\
\tau_y &\sim \text{Uni}(0, 100)\\
\nu_\beta &\sim \text{Uni}(0, 100)\\
\tau_\beta &\sim \text{Uni}(0, 100)\\
\end{aligned}
$$

Where $k$ index the polls or the imputed polls, $i$ index the congressional district, and $t$ index the polls date. $i[k]$ represents the $k^{th}$ poll's corresponding congressional district index, and $t[k]$ represents the $k^{th}$ poll's corresponding date index.


```{r load history data, include=F}
history <- readRDS("ncvhis_Statewide_small.rds")
history$election_lbl <- as.Date(history$election_lbl,  "%m/%d/%y")
history$cong_dist_abbrv[history$county_desc == "BUNCOMBE"] <- 11
history$cong_dist_abbrv[history$county_desc == "ALLEGHANY"] <- 5
history$cong_dist_abbrv[history$county_desc == "DAVIDSON"] <- 13
history$cong_dist_abbrv[history$county_desc == "BUNCOMBE"] <- 11
history$cong_dist_abbrv[history$county_desc == "CLAY"] <- 11
history$cong_dist_abbrv[history$county_desc == "POLK"] <- 11
history$cong_dist_abbrv[history$county_desc == "CALDWELL"] <- 5
history$cong_dist_abbrv[history$county_desc == "VANCE"] <- 1
history$cong_dist_abbrv[history$county_desc == "RICHMOND"] <- 9
history$cong_dist_abbrv[history$county_desc == "MECKLENBURG"] <- 12
history$cong_dist_abbrv[history$county_desc == "ROBESON"] <- 9
history$cong_dist_abbrv[history$county_desc == "CUMBERLAND"] <- 8
history$cong_dist_abbrv[history$county_desc == "CABARRUS"] <- 8
history$cong_dist_abbrv[history$county_desc == "ROWAN"] <- 13
history$cong_dist_abbrv[history$county_desc == "JOHNSTON"] <- 7
history$cong_dist_abbrv[history$county_desc == "COLUMBUS"] <- 7
history$cong_dist_abbrv[history$county_desc == "BLADEN"] <- 7
history$cong_dist_abbrv[history$county_desc == "DURHAM"] <- 4
history$cong_dist_abbrv[history$county_desc == "CARTERET"] <- 3
history$cong_dist_abbrv[history$county_desc == "CHATHAM"] <- 4
history$cong_dist_abbrv[history$county_desc == "RANDOLPH"] <- 6
history$cong_dist_abbrv[history$county_desc == "GUILFORD"] <- 6
history$cong_dist_abbrv[history$county_desc == "CRAVEN"] <- 3
history$cong_dist_abbrv[history$county_desc == "DUPLIN"] <- 3
history$cong_dist_abbrv[history$county_desc == "GRANVILLE"] <- 4
history$cong_dist_abbrv[history$county_desc == "HALIFAX"] <- 1
history$cong_dist_abbrv[history$county_desc == "NORTHAMPTON"] <- 1
history$cong_dist_abbrv[history$county_desc == "MONTGOMERY"] <- 8
history$cong_dist_abbrv[history$county_desc == "ONSLOW"] <- 3
history$cong_dist_abbrv[history$county_desc == "ORANGE"] <- 4
history$cong_dist_abbrv[history$county_desc == "PAMLICO"] <- 3
history$cong_dist_abbrv[history$county_desc == "PENDER"] <- 7
history$cong_dist_abbrv[history$county_desc == "PERQUIMANS"] <- 3
history$cong_dist_abbrv[history$county_desc == "PITT"] <- 1
history$cong_dist_abbrv[history$county_desc == "RUTHERFORD"] <- 11
history$cong_dist_abbrv[history$county_desc == "STANLY"] <- 8
history$cong_dist_abbrv[history$county_desc == "YADKIN"] <- 10
history$cong_dist_abbrv[history$county_desc == "ANSON"] <- 9
history$cong_dist_abbrv[history$county_desc == "BEAUFORT"] <- 3
history$cong_dist_abbrv[history$county_desc == "BERTIE"] <- 1
history$cong_dist_abbrv[history$county_desc == "BRUNSWICK"] <- 7
history$cong_dist_abbrv[history$county_desc == "CAMDEN"] <- 11
history$cong_dist_abbrv[history$county_desc == "AVERY"] <- 5
history$cong_dist_abbrv[history$county_desc == "CASWELL"] <- 13
history$cong_dist_abbrv[history$county_desc == "CATAWBA"] <- 10
history$cong_dist_abbrv[history$county_desc == "CHEROKEE"] <- 11

noncompetitive <- c(1,4,5,6,10,12)
district <- c()
for(i in 1:length(noncompetitive))
{
  dist_num <- noncompetitive[i]
  
  dist_data <- history %>% filter(cong_dist_abbrv == dist_num)
  num_DEM <- nrow(dist_data %>% filter(voted_party_cd == "DEM"))
  num_REP <- nrow(dist_data %>% filter(voted_party_cd == "REP"))
  REP_pct <- num_REP / (num_REP + num_DEM)
  district[i] <- REP_pct * 100
}

district <- c(26.83741, 22.78081, 68.55712, 45.72268, 70.02141, 36.26725)

```

```{r make hypothetical house poll data, include=F}
house <- read_csv("house_polls.csv")
house <- house %>% filter(state == "North Carolina")
house$days_to_election = as.double(as.Date(house$election_date, "%m/%d/%Y") - as.Date(house$end_date, "%m/%d/%Y"))
house <- house %>% filter(candidate_party %in% c("DEM","REP"))

house$y = ifelse(house$candidate_party == "REP", house$pct, 100-house$pct) # support ratio for rep

days <- max(house$days_to_election)
total_day <- NULL
total_pct <- NULL
total_cong <- NULL
for(i in 1:length(district))
{
  seq_day <- seq(as.double(days), 1, -1)
  seq_pct <- rep(district[i], length(seq_day))
  seq_cong <- rep(paste("district", noncompetitive[i]),length(seq_day))
  total_seq <- rbind(total_day, as.matrix(seq_day))
  total_pct <- rbind(total_pct, as.matrix(seq_pct))
  total_cong <- rbind(total_cong, as.matrix(seq_cong))
}

append_house <- data.frame(total_seq, total_pct, total_cong)
names(append_house) <- c("days_to_election", "pct", "seat_name")

house <- dplyr::bind_rows(house, append_house)

cong_dist <- house$seat_name %>% unique


house$y = ifelse( is.na(house$y), house$pct, house$y) # support ratio for rep
house$r <- match(house$seat_name, cong_dist)
house$t <- house$days_to_election + 1 #WHY PLUS ONE?

house_y<- house$y
house_r<- house$r
house_t<- house$t

N_polls <- house_y %>% length
N_states <- cong_dist %>% length
N_days <- house_t %>% max


```



```{r mv_model-house, include=F}
model_house <- function(){
  for(k in 1:N_polls)
  {
    y[k] ~ dnorm(p[k],1/sigma2_y[r[k]]) #note no longer binomial
    p[k] = theta[r[k],t[k]]
  }
  for(j in 2:N_days)
  {
    theta[1:N_states,j] ~ dmnorm(theta[1:N_states,j-1],Phi)
  }
  Phi ~ dwish(I_states,N_states+1) #fill in wishart parameters, google JAGS wishart distribution should turn it up
  Sigma = inverse(Phi)
  #which, Phi or Sigma is the covariance and which is the precision? 
  
  #optional: theta[1:N_states,1] ~ dmnorm(mu0,s0) #define mu0 and s0 in your jags_data object
  
  #Use your hierarhciacl prior for sigma2_y from before 
  for(j in 1:N_states){
      sigma2_y[j] = 1/sigma2_y_inv[j]
      sigma2_y_inv[j] ~ dgamma(nu_y,nu_y*tau_y) 
      
      theta[j,1] ~ dnorm(mu0,pow(sigma2_0,-1))
  }
  nu_y ~ dunif(0,100)
  tau_y ~ dunif(0,100)
  
  nu_beta ~ dunif(0,100)
  tau_beta ~ dunif(0,100)
  
  mu0 ~ dnorm(50,pow(7.5,-2))
  sigma2_0 = 1/sigma2_0_inv
  sigma2_0_inv ~ dgamma(.5,.5)
}
```





```{r run jags house,eval=TRUE,cache=TRUE, include=F}

house_jags_data <- list(y=house_y,t=house_t,r=house_r,
                  N_polls=N_polls,N_states=N_states,N_days=N_days)

house_jags_data$I_states <- diag(N_states)

#be sure to add your added parameters to parameters.to.save
jags_sims_house <- jags(data = house_jags_data,
                          model.file = model_house,
                          parameters.to.save = c("theta","Sigma","p","sigma2_y"),
                          n.iter = 10000)
```

```{r plot house for all congression dist, eval=F, echo=F}
house_poll_plot_data <- tibble(y=house_jags_data$y,
                                 t=house_jags_data$t %>% as.integer(),
                                 cong_dist = cong_dist[house_jags_data$r])

house_beta_plot_data <- lapply(1:N_states,function(st){
  sims <- jags_sims_house$BUGSoutput$sims.list$theta[,st,]
  data.frame(cong_dist = cong_dist[st],
             t=1:N_days,mean=colMeans(sims),
             lb=apply(sims,2,quantile,probs=.025),
             ub=apply(sims,2,quantile,probs=.975))}) %>% 
  bind_rows()

left_join(house_beta_plot_data,house_poll_plot_data) %>% 
  ggplot(aes(x=t)) + 
  geom_line(aes(y=mean)) + 
  geom_ribbon(aes(ymin = lb,ymax = ub),alpha = .2) + 
  geom_point(aes(y=y)) + 
  scale_x_reverse() + 
  facet_wrap(~ cong_dist)


```


```{r plot house prob for all cong_dist, eval=F, echo=F}
house_poll_plot_data$p <- jags_sims_house$BUGSoutput$mean$p
house_poll_plot_data$sigma2y <- jags_sims_house$BUGSoutput$mean$sigma2_y[house_jags_data$r]
house_poll_plot_data$binom_v <- (house_poll_plot_data$p)*(100-house_poll_plot_data$p)/ senator$sample_size !!!


house_poll_plot_data %>%
  ggplot(aes(x=binom_v)) +
  geom_density() +
  geom_vline(aes(xintercept = sigma2y),color = "red") +
  facet_wrap(~ cong_dist)


```


```{r house result, include=F}


house_sims <- jags_sims_house$BUGSoutput$sims.list$theta[,,1]
colnames(house_sims) <- cong_dist
house_all_cong_dist_prob <- as.data.frame(house_sims %>% {.>50} %>% colMeans())

# target <- c("district 1", 
#             "district 2", 
#             "district 3", 
#             "district 4",
#             "district 5", 
#             "district 6", 
#             "district 7", 
#             "district 8",
#             "district 9", 
#             "district 10", 
#             "district 11", 
#             "district 12",
#             "district 13")

names(house_all_cong_dist_prob) <- "Winning Probability"

house_all_cong_dist_prob <- kable(house_all_cong_dist_prob, caption = "House All Congressional District Winning Probability (REP)")
house_all_cong_dist_share_interval <- kable(colQuantiles(house_sims, probs=c(0.025, 0.5, 0.975)), caption = "house All State Vote Share Percentage Interval Estimate (REP)")
```

```{r output all results, echo=F}
swing_state_traj #plot swing state DEM party
president_var #plot president variance
swing_state_prob #kable swing state probility of biden win
ec_dis #plot electorial college vote distribution
ec_interval #kable of ec vote CI

swing_state_win_prob # Kable Swing State winning probability
swing_state_share_interval_president # Kable Swing State share interval president



senator_trend # plot senator trend for 47 states
senator_var # plot senator variance for 47 states

senator_all_state_prob # kable all state senate winning probability
swing_state_share_interval # kable all state senate vote share interval


nc_senate_share_qt # Kable Tom Tillis share pt
nc_senate_share # plot Tom Tillis Share
tom_win_prob # Tom Win Prob

rep_senate_prob # probability REP controls senate
senate_dis # plot total REP senators


house_all_cong_dist_prob # Kable congressional district REP winning probability
house_all_cong_dist_share_interval # Kable congressional district REP share pct interval
```

```{r}
swing_state_share_interval_president
```

